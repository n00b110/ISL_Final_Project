# -*- coding: utf-8 -*-
"""Copy of MNIST_Sequence.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qVsAmDLa24GHdF89mwFDdGd9uCczez7M
"""

# !pip install -U portalocker>=2.0.0

"""# Neural Nets Task

### PyTorch FC ANN MNIST Implementation.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

"""## Change seed and batch size here"""

# Transformations --> this is a "pre-processing step" that's typical for image processing methods
SEED = 6727
torch.manual_seed(SEED)

# Define the transformations
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert PIL image or numpy.ndarray to tensor
    transforms.Normalize((0.5,), (0.5,))  # Normalize data to range [-1, 1]
])

# Load the MNIST dataset
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Change batch size here from 128 to provided values

# Create DataLoaders with a fixed seed for reproducible shuffling
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, worker_init_fn=lambda _: torch.manual_seed(SEED))
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)

# Confirming the setup
print(f"Number of training samples: {len(train_loader.dataset)}")
print(f"Number of testing samples: {len(test_loader.dataset)}")

# Mapping the labels for the MNIST dataset -- later we'll see that this using the "keras to_categorical" method as discussed in class
labels_map = {
    0: "0", 1: "1", 2: "2", 3: "3", 4: "4",
    5: "5", 6: "6", 7: "7", 8: "8", 9: "9"
}

#This cell is designed to display a few images from the dataset
#It isn't necessary to run this, but it can help give a better idea of the challanges your model will face
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3

# Displaying figures from the dataset randomly
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()
    img, label = train_dataset[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()

"""## Example topology with sigmoid activation function and learning rate as 0.1

###Experiment 1 :-
"""



#Here we define the model parameters -- the general strucutre as provided here will produce a fully connected network [28x28] --> 32 --> 16 --> 10
class MLP(nn.Module): #MLP stands for "Multi-Layer Perceptron"
    def __init__(self): #this initializes the structure of the network
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 256) # add or remove layer here so, fc1, fc2 and so on must be continuous [look fo the numbers]
        self.fc2 = nn.Linear(256 , 96)
        self.fc4 = nn.Linear(96 , 32)
        self.fc3 = nn.Linear(32, 10) ## 10 output features because MNIST has 10 target classes

    def forward(self, x): #this modifies the elements of the intial structure defined above
        x = x.view(-1, 28 * 28) #the array is sent in as a vector
        x = torch.sigmoid(self.fc1(x)) ## Applying sigmoid activation for the first layer replace with 'relu'
        x = torch.sigmoid(self.fc2(x)) ## Applying sigmoid activation for the second layer replace with 'relu'
        x = torch.sigmoid(self.fc4(x))
        x = self.fc3(x) ## no modifications to the activation of the output layer
        return x

# Initializing the neural network
model = MLP()


# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training the neural network
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate training accuracy
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

        if i % 100 == 99:  # Print every 100 mini-batches
            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.4f}')
            running_loss = 0.0

# Calculate and print final training accuracy
final_training_accuracy = 100 * correct_train / total_train
print(f'Finished Training. Final Training Accuracy: {final_training_accuracy:.2f}%')

# Evaluating the model on the test set
model.eval()
correct_test = 0
total_test = 0

with torch.no_grad():
    for data in test_loader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total_test += labels.size(0)
        correct_test += (predicted == labels).sum().item()

# Calculate and print test accuracy
test_accuracy = 100 * correct_test / total_test
print(f'Accuracy on test set: {test_accuracy:.2f}%')

# Visualizing a single prediction
image_index = 27
test_image, test_label = test_dataset[image_index]

with torch.no_grad():
    model.eval()
    output = model(test_image.unsqueeze(0))
    _, predicted_label = torch.max(output, 1)

test_image_numpy = test_image.squeeze().numpy()

plt.imshow(test_image_numpy, cmap='gray')
plt.title(f'Predicted Label: {predicted_label.item()}, Actual Label: {test_label}')
plt.axis('off')
plt.show()